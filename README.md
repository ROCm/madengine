# madengine

[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://python.org)
[![CI](https://img.shields.io/badge/CI-GitHub%20Actions-green.svg)](https://github.com/ROCm/madengine/actions)
[![Code Style](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![Version](https://img.shields.io/badge/version-2.0-brightgreen.svg)](CHANGELOG.md)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

> **AI model automation and benchmarking platform for local and distributed execution**

madengine is a modern CLI tool for running Large Language Models (LLMs) and Deep Learning models across local and distributed environments. Built for the [MAD (Model Automation and Dashboarding)](https://github.com/ROCm/MAD) ecosystem, it provides seamless execution from single GPUs to multi-node clusters.

## ğŸ“– Table of Contents

- [Key Features](#-key-features)
- [Quick Start](#-quick-start)
- [Commands](#-commands)
- [Documentation](#-documentation)
- [Architecture](#-architecture)
- [Feature Matrix](#-feature-matrix)
- [Usage Examples](#-usage-examples)
- [Model Discovery](#-model-discovery)
- [Performance Profiling](#-performance-profiling)
- [Reporting and Database](#-reporting-and-database)
- [Installation](#-installation)
- [Tips & Best Practices](#-tips--best-practices)
- [Contributing](#-contributing)
- [License](#-license)
- [Links & Resources](#-links--resources)

## âœ¨ Key Features

- **ğŸš€ Modern CLI** - Rich terminal output with Typer and Rich
- **ğŸ¯ Simple Deployment** - Run locally or deploy to Kubernetes/SLURM via configuration
- **ğŸ”§ Distributed Launchers** - Full support for torchrun, DeepSpeed, Megatron-LM, TorchTitan, vLLM, SGLang
- **ğŸ³ Container-Native** - Docker-based execution with GPU support (ROCm, CUDA)
- **ğŸ“Š Performance Tools** - Integrated profiling with rocprof, rocblas, MIOpen, RCCL tracing
- **âš™ï¸ Intelligent Defaults** - Minimal K8s configs with automatic preset application

## ğŸš€ Quick Start

```bash
# Install madengine
pip install git+https://github.com/ROCm/madengine.git

# Clone MAD package (required for models)
git clone https://github.com/ROCm/MAD.git && cd MAD

# Discover available models
madengine discover --tags dummy

# Run locally
madengine run --tags dummy \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'
```

**Results saved to `perf_entry.csv`**

## ğŸ“‹ Commands

madengine provides five main commands for model automation and benchmarking:

| Command | Description | Use Case |
|---------|-------------|----------|
| **[discover](#-model-discovery)** | Find available models | Model exploration and validation |
| **[build](#building-images)** | Build Docker images | Create containerized models |
| **[run](#-usage-examples)** | Execute models | Local and distributed execution |
| **[report](docs/cli-reference.md#report---generate-reports)** | Generate HTML reports | Convert CSV to viewable reports |
| **[database](docs/cli-reference.md#database---upload-to-mongodb)** | Upload to MongoDB | Store results in database |

**Quick Start:**

```bash
# Discover models
madengine discover --tags dummy

# Build image
madengine build --tags dummy \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# Run model
madengine run --tags dummy \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# Generate report
madengine report to-html --csv-file perf_entry.csv

# Upload results
madengine database --csv-file perf_entry.csv --db mydb --collection results
```

For detailed command options, see the **[CLI Command Reference](docs/cli-reference.md)**.

## ğŸ“š Documentation

| Guide | Description |
|-------|-------------|
| [Installation](docs/installation.md) | Complete installation instructions |
| [Usage Guide](docs/usage.md) | Commands, workflows, and examples |
| **[CLI Reference](docs/cli-reference.md)** | **Detailed command options and examples** |
| [Deployment](docs/deployment.md) | Kubernetes and SLURM deployment |
| [Configuration](docs/configuration.md) | Advanced configuration options |
| [Batch Build](docs/batch-build.md) | Selective builds for CI/CD |
| [Launchers](docs/launchers.md) | Distributed training frameworks |
| [Profiling](docs/profiling.md) | Performance analysis tools |
| [Contributing](docs/contributing.md) | How to contribute |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               madengine                         â”‚
â”‚          (build, run, discover)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚  Build  â”‚   â”‚   Run   â”‚   â”‚Discover â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚             â”‚              â”‚
        â–¼             â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Orchestration Layer                   â”‚
â”‚   (BuildOrchestrator / RunOrchestrator)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚              â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚  Local  â”‚   â”‚   K8s   â”‚   â”‚  SLURM  â”‚
   â”‚Containerâ”‚   â”‚  Deploy â”‚   â”‚  Deploy â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼              â–¼              â–¼
   torchrun      DeepSpeed        vLLM
   TorchTitan    Megatron-LM      SGLang
```

## ğŸ¯ Feature Matrix

### Supported Launchers & Infrastructure

| Launcher | Local | Kubernetes | SLURM | Type | Key Features |
|----------|-------|-----------|-------|------|--------------|
| **torchrun** | âœ… | âœ… | âœ… | Training | PyTorch DDP/FSDP, elastic training |
| **DeepSpeed** | âœ… | âœ… | âœ… | Training | ZeRO optimization, pipeline parallelism |
| **Megatron-LM** | âœ… | âœ… | âœ… | Training | Tensor+Pipeline parallel, large transformers |
| **TorchTitan** | âœ… | âœ… | âœ… | Training | FSDP2+TP+PP+CP, Llama 3.1 (8B-405B) |
| **vLLM** | âœ… | âœ… | âœ… | Inference | v1 engine, PagedAttention, Ray cluster |
| **SGLang** | âœ… | âœ… | âœ… | Inference | RadixAttention, structured generation |
| **SGLang Disagg** | âŒ | âœ… | âœ… | Inference | Disaggregated prefill/decode, Mooncake, 3+ nodes |

**Note:** All launchers support single-GPU, multi-GPU (single node), and multi-node (where infrastructure allows). See [Launchers Guide](docs/launchers.md) for details.

### Parallelism Capabilities

| Launcher | Data Parallel | Tensor Parallel | Pipeline Parallel | Context Parallel | Ray Cluster | Architecture |
|----------|--------------|----------------|-------------------|-----------------|-------------|--------------|
| **torchrun** | âœ… DDP/FSDP | âŒ | âŒ | âŒ | âŒ | Unified |
| **DeepSpeed** | âœ… ZeRO | âŒ | âœ… | âŒ | âŒ | Unified |
| **Megatron-LM** | âœ… | âœ… | âœ… | âŒ | âŒ | Unified |
| **TorchTitan** | âœ… FSDP2 | âœ… | âœ… | âœ… | âŒ | Unified |
| **vLLM** | âŒ | âœ… | âœ… | âŒ | âœ… Multi-node | Unified |
| **SGLang** | âŒ | âœ… | âŒ | âŒ | âœ… Multi-node | Unified |
| **SGLang Disagg** | âŒ | âœ… | âœ… (via disagg) | âŒ | âœ… Multi-node | Disaggregated |

### Infrastructure Capabilities

| Feature | Local | Kubernetes | SLURM |
|---------|-------|-----------|-------|
| **Execution** | Docker containers | K8s Jobs | SLURM jobs |
| **Multi-Node** | âŒ | âœ… Indexed Jobs | âœ… Job arrays |
| **Resource Mgmt** | Manual | Declarative (YAML) | Batch scheduler |
| **Monitoring** | Docker logs | kubectl/dashboard | squeue/scontrol |
| **Auto-scaling** | âŒ | âœ… | âŒ |
| **Network** | Host | CNI plugin | InfiniBand/Ethernet |

## ğŸ’» Usage Examples

### Local Execution

```bash
# Single GPU
madengine run --tags dummy \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# Multi-GPU with torchrun (DDP/FSDP)
madengine run --tags model \
  --additional-context '{
    "gpu_vendor": "AMD",
    "guest_os": "UBUNTU",
    "docker_gpus": "0,1,2,3",
    "distributed": {
      "launcher": "torchrun",
      "nproc_per_node": 4
    }
  }'

# With DeepSpeed (ZeRO optimization)
madengine run --tags model \
  --additional-context '{
    "gpu_vendor": "AMD",
    "guest_os": "UBUNTU",
    "docker_gpus": "all",
    "distributed": {
      "launcher": "deepspeed",
      "nproc_per_node": 8
    }
  }'
```

### Kubernetes Deployment

```bash
# Minimal config (auto-defaults applied)
madengine run --tags model \
  --additional-context '{"k8s": {"gpu_count": 2}}'

# Multi-node inference with vLLM
madengine run --tags model \
  --additional-context '{
    "k8s": {
      "namespace": "ml-team",
      "gpu_count": 8
    },
    "distributed": {
      "launcher": "vllm",
      "nnodes": 2,
      "nproc_per_node": 4
    }
  }'

# SGLang with structured generation
madengine run --tags model \
  --additional-context '{
    "k8s": {"gpu_count": 4},
    "distributed": {
      "launcher": "sglang",
      "nproc_per_node": 4
    }
  }'
```

### SLURM Deployment

```bash
# Build phase (local or CI)
madengine build --tags model \
  --registry gcr.io/myproject \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# Deploy phase (on SLURM login node)
madengine run --manifest-file build_manifest.json \
  --additional-context '{
    "slurm": {
      "partition": "gpu",
      "nodes": 4,
      "gpus_per_node": 8,
      "time": "24:00:00"
    },
    "distributed": {
      "launcher": "torchtitan",
      "nnodes": 4,
      "nproc_per_node": 8
    }
  }'
```

### Common Workflows

**Development â†’ Testing â†’ Production:**

```bash
# 1. Develop locally with single GPU
madengine run --tags model \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# 2. Test multi-GPU locally
madengine run --tags model \
  --additional-context '{
    "gpu_vendor": "AMD",
    "guest_os": "UBUNTU",
    "docker_gpus": "0,1",
    "distributed": {"launcher": "torchrun", "nproc_per_node": 2}
  }'

# 3. Build and push to registry
madengine build --tags model \
  --registry docker.io/myorg \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# 4. Deploy to Kubernetes
madengine run --manifest-file build_manifest.json
```

**CI/CD Pipeline:**

```bash
# Batch build (selective rebuilds)
madengine build --batch-manifest batch.json \
  --registry docker.io/myorg

# Run tests
madengine run --manifest-file build_manifest.json \
  --additional-context '{"k8s": {"namespace": "ci-test"}}'

# Generate and email reports
madengine report to-email --directory ./results --output ci_report.html

# Upload to database
madengine database --csv-file perf_entry.csv \
  --database-name ci_db --collection-name test_results
```

See [Usage Guide](docs/usage.md), [Configuration Guide](docs/configuration.md), and [CLI Reference](docs/cli-reference.md) for more examples.

### Building Images

```bash
# Build single model
madengine build --tags dummy \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# Build with registry (for distributed deployment)
madengine build --tags model1 model2 \
  --registry localhost:5000 \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'

# Build for multiple GPU architectures
madengine build --tags model \
  --target-archs gfx908 gfx90a gfx942 \
  --registry gcr.io/myproject

# Batch build mode (selective builds for CI/CD)
madengine build --batch-manifest examples/build-manifest/batch.json \
  --registry docker.io/myorg

# Clean rebuild (no Docker cache)
madengine build --tags model --clean-docker-cache \
  --additional-context '{"gpu_vendor": "AMD", "guest_os": "UBUNTU"}'
```

**Output:** Creates `build_manifest.json` with built image names and configurations.

See [Batch Build Guide](docs/batch-build.md) and examples in [`examples/build-manifest/`](examples/build-manifest/).

## ğŸ” Model Discovery

madengine discovers models from the MAD package using three methods:

```bash
# Root models (models.json)
madengine discover --tags pyt_huggingface_bert

# Directory-specific (scripts/{dir}/models.json)
madengine discover --tags dummy2:dummy_2

# Dynamic with parameters (scripts/{dir}/get_models_json.py)
madengine discover --tags dummy3:dummy_3:batch_size=512
```

## ğŸ“Š Performance Profiling

madengine includes integrated profiling tools for AMD ROCm:

```bash
# GPU profiling with rocprof
madengine run --tags model \
  --additional-context '{
    "gpu_vendor": "AMD",
    "guest_os": "UBUNTU",
    "tools": [{"name": "rocprof"}]
  }'

# Library tracing (rocBLAS, MIOpen, Tensile, RCCL)
madengine run --tags model \
  --additional-context '{"tools": [{"name": "rocblas_trace"}]}'

# Power and VRAM monitoring
madengine run --tags model \
  --additional-context '{"tools": [
    {"name": "gpu_info_power_profiler"},
    {"name": "gpu_info_vram_profiler"}
  ]}'

# Multiple tools (stackable)
madengine run --tags model \
  --additional-context '{"tools": [
    {"name": "rocprof"},
    {"name": "rocblas_trace"},
    {"name": "gpu_info_power_profiler"}
  ]}'
```

**Available Tools:**

| Tool | Purpose | Output |
|------|---------|--------|
| `rocprof` | GPU kernel profiling | Kernel timings, occupancy |
| `rocblas_trace` | rocBLAS library calls | Function calls, arguments |
| `miopen_trace` | MIOpen library calls | Conv/pooling operations |
| `tensile_trace` | Tensile GEMM library | Matrix multiply details |
| `rccl_trace` | RCCL collective ops | Communication patterns |
| `gpu_info_power_profiler` | GPU power consumption | Power usage over time |
| `gpu_info_vram_profiler` | GPU memory usage | VRAM utilization |

See [Profiling Guide](docs/profiling.md) for detailed usage and analysis.

## ğŸ“Š Reporting and Database

### Generate Reports

Convert performance CSV files to HTML reports:

```bash
# Single CSV to HTML
madengine report to-html --csv-file perf_entry.csv

# Consolidated email report (all CSVs in directory)
madengine report to-email --directory ./results --output summary.html
```

### Upload to Database

Store performance results in MongoDB:

```bash
# Set MongoDB connection
export MONGO_HOST=mongodb.example.com
export MONGO_PORT=27017
export MONGO_USER=myuser
export MONGO_PASSWORD=mypassword

# Upload CSV to MongoDB
madengine database --csv-file perf_entry.csv \
  --database-name performance_db \
  --collection-name model_runs
```

**Use Cases:**
- Track performance over time
- Compare results across different configurations
- Build performance dashboards
- Automated CI/CD reporting

See [CLI Reference](docs/cli-reference.md) for complete options.

## ğŸ“¦ Installation

```bash
# Basic installation
pip install git+https://github.com/ROCm/madengine.git

# With Kubernetes support
pip install "madengine[kubernetes] @ git+https://github.com/ROCm/madengine.git"

# Development installation
git clone https://github.com/ROCm/madengine.git
cd madengine && pip install -e ".[dev]"
```

See [Installation Guide](docs/installation.md) for detailed instructions.

## ğŸ’¡ Tips & Best Practices

### General Usage

- **Use configuration files** for complex setups instead of long command lines
- **Test locally first** with single GPU before scaling to multi-node
- **Enable verbose logging** (`--verbose`) when debugging issues
- **Use `--live-output`** for real-time monitoring of long-running operations

### Build & Deployment

- **Separate build and run phases** for distributed deployments
- **Use registries** for multi-node execution (K8s/SLURM)
- **Use batch build mode** for CI/CD to optimize build times
- **Specify `--target-archs`** when building for multiple GPU architectures

### Performance

- **Start with small timeouts** and increase as needed
- **Use profiling tools** to identify bottlenecks
- **Monitor GPU utilization** with `gpu_info_power_profiler`
- **Profile library calls** with rocBLAS/MIOpen tracing

### Troubleshooting

```bash
# Check model is available
madengine discover --tags your_model

# Verbose output for debugging
madengine run --tags model --verbose --live-output

# Keep container alive for inspection
madengine run --tags model --keep-alive

# Clean rebuild if build fails
madengine build --tags model --clean-docker-cache --verbose
```

## ğŸ¤ Contributing

We welcome contributions! See [Contributing Guide](docs/contributing.md) for details.

```bash
git clone https://github.com/ROCm/madengine.git
cd madengine
python3 -m venv venv && source venv/bin/activate
pip install -e ".[dev]"
pytest
```

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details.

## ğŸ”— Links & Resources

### Documentation
- **[CLI Reference](docs/cli-reference.md)** - Complete command options
- **[Usage Guide](docs/usage.md)** - Workflows and examples
- **[Deployment Guide](docs/deployment.md)** - Kubernetes/SLURM deployment
- **[Configuration Guide](docs/configuration.md)** - Advanced configuration
- **[All Docs](docs/)** - Complete documentation index

### External Resources
- **MAD Package**: https://github.com/ROCm/MAD
- **Issues & Support**: https://github.com/ROCm/madengine/issues
- **ROCm Documentation**: https://rocm.docs.amd.com/

### Getting Help

**Command Help:**
```bash
madengine --help                    # Main help
madengine <command> --help          # Command-specific help
madengine report --help             # Sub-app help
madengine report to-html --help     # Sub-command help
```

**Quick Checks:**
```bash
# Verify installation
madengine --version

# Discover available models
madengine discover

# Check specific model
madengine discover --tags your_model --verbose
```

**Troubleshooting:**
- Check [CLI Reference](docs/cli-reference.md) for all command options
- Enable `--verbose` flag for detailed error messages
- See [Usage Guide](docs/usage.md) troubleshooting section
- Report issues: https://github.com/ROCm/madengine/issues

---

## âš ï¸ Migration Notice (v2.0.0+)

The CLI has been unified! Starting from v2.0.0:
- âœ… Use `madengine` (unified modern CLI with K8s, SLURM, distributed support)
- âŒ Legacy v1.x CLI has been removed

---

**Code Quality**: Clean codebase with no dead code, comprehensive test coverage, and following Python best practices.
