{
  "_comment": "Minimal TorchTitan SLURM configuration (1 node x 8 GPUs)",
  "_description": "Llama 3.1 8B pre-training with Tensor Parallelism",
  "_reference": "https://github.com/pytorch/torchtitan",
  
  "slurm": {
    "partition": "amd-rccl",
    "nodes": 1,
    "gpus_per_node": 8,
    "time": "24:00:00",
    "mem": "256G"
  },
  
  "distributed": {
    "launcher": "torchtitan"
  }
}

