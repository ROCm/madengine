{
  "_comment": "Megatron-LM Multi-Node Training Configuration",
  "_description": "Large-scale transformer training with Megatron-LM on SLURM",
  "_use_case": "Multi-node Megatron-LM training with tensor and pipeline parallelism",
  "_reference": "https://github.com/NVIDIA/Megatron-LM",
  
  "gpu_vendor": "AMD",
  "guest_os": "UBUNTU",
  
  "slurm": {
    "partition": "gpu",
    "account": "research",
    "nodes": 4,
    "gpus_per_node": 8,
    "time": "24:00:00",
    "mem": "256G"
  },
  
  "distributed": {
    "launcher": "megatron",
    "nnodes": 4,
    "nproc_per_node": 8,
    "master_port": 29500
  },
  
  "env_vars": {
    "OMP_NUM_THREADS": "16",
    "NCCL_DEBUG": "INFO",
    "NCCL_IB_DISABLE": "0"
  },
  
  "debug": false
}

