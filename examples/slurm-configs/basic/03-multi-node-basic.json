{
  "_comment": "Multi-Node (2 nodes, 8 GPUs each) - SLURM Configuration",
  "_description": "Configuration for distributed training across 2 nodes with 8 GPUs per node (16 GPUs total)",
  "_use_case": "Multi-node distributed training for large models",
  "_note": "Target is auto-detected as 'slurm' from presence of 'slurm' config section",
  
  "gpu_vendor": "AMD",
  "guest_os": "UBUNTU",
  
  "slurm": {
    "partition": "amd-rccl",
    "nodes": 2,
    "gpus_per_node": 8,
    "time": "24:00:00",
    "output_dir": "./slurm_output",
    "exclusive": true,
    "network_interface": "eth0"
  },
  
  "distributed": {
    "backend": "nccl",
    "port": 29500
  },
  
  "env_vars": {
    "NCCL_DEBUG": "WARN",
    "NCCL_DEBUG_SUBSYS": "INIT,NET",
    "NCCL_IB_DISABLE": "1",
    "NCCL_SOCKET_IFNAME": "eth0",
    "TORCH_NCCL_HIGH_PRIORITY": "1",
    "GPU_MAX_HW_QUEUES": "2",
    "TORCH_NCCL_ASYNC_ERROR_HANDLING": "1",
    "NCCL_TIMEOUT": "600",
    "HSA_ENABLE_SDMA": "0",
    "OMP_NUM_THREADS": "8",
    "MIOPEN_FIND_MODE": "1",
    "MIOPEN_USER_DB_PATH": "/tmp/.miopen",
    "HSA_FORCE_FINE_GRAIN_PCIE": "1",
    "RCCL_ENABLE_HIPGRAPH": "0"
  },
  
  "debug": false
}

