{
  "_comment": "Megatron-LM Minimal Config - Dedicated launcher support",
  "_description": "Megatron-LM with automated tensor/pipeline parallelism setup",
  "_use_case": "Large-scale transformer training with Megatron-LM on Kubernetes",
  
  "gpu_vendor": "AMD",
  "guest_os": "UBUNTU",
  
  "k8s": {
    "gpu_count": 2,
    "namespace": "default",
    "memory": "32Gi",
    "cpu": "16"
  },
  
  "distributed": {
    "launcher": "megatron",
    "nnodes": 1,
    "nproc_per_node": 2
  },
  
  "env_vars": {
    "OMP_NUM_THREADS": "8"
  }
}
