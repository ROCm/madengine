#!/bin/bash
#SBATCH --job-name=madengine-{{ model_name }}
#SBATCH --output={{ output_dir }}/madengine-{{ model_name }}_%j_%t.out
#SBATCH --error={{ output_dir }}/madengine-{{ model_name }}_%j_%t.err
#SBATCH --partition={{ partition }}
#SBATCH --nodes={{ nodes }}
#SBATCH --ntasks={{ nodes }}
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node={{ gpus_per_node }}
#SBATCH --time={{ time_limit }}
{% if exclusive %}
#SBATCH --exclusive
{% endif %}
{% if qos %}
#SBATCH --qos={{ qos }}
{% endif %}
{% if account %}
#SBATCH --account={{ account }}
{% endif %}

# =============================================================================
# SLURM Job Configuration Generated by madengine-cli
# Model: {{ model_name }}
# Deployment: {{ nodes }} nodes x {{ gpus_per_node }} GPUs
# =============================================================================

# Load required modules
{% for module in modules %}
module load {{ module }}
{% endfor %}

# =============================================================================
# Environment Setup (Standard ML Environment Variables)
# =============================================================================

# Distributed training environment (auto-configured from SLURM)
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT={{ master_port | default(29500) }}
export WORLD_SIZE=$SLURM_NTASKS
export RANK=$SLURM_PROCID
export LOCAL_RANK=$SLURM_LOCALID
export NNODES={{ nodes }}
export GPUS_PER_NODE={{ gpus_per_node }}

# GPU visibility (ROCm/CUDA)
export ROCR_VISIBLE_DEVICES=$(seq -s, 0 $(({{ gpus_per_node }}-1)))
export CUDA_VISIBLE_DEVICES=$ROCR_VISIBLE_DEVICES

# Network configuration
{% if network_interface %}
export NCCL_SOCKET_IFNAME={{ network_interface }}
export GLOO_SOCKET_IFNAME={{ network_interface }}
{% endif %}

# Distributed backend configuration
{% if distributed_backend %}
export DISTRIBUTED_BACKEND={{ distributed_backend }}
{% endif %}

# Application-specific environment variables
{% for key, value in env_vars.items() %}
export {{ key }}="{{ value }}"
{% endfor %}

# madengine environment
export MAD_SLURM_JOB_ID=$SLURM_JOB_ID
export MAD_NODE_RANK=$SLURM_NODEID
export MAD_TOTAL_NODES={{ nodes }}

# =============================================================================
# Workspace Setup
# =============================================================================

{% if shared_workspace %}
# Use shared workspace (NFS/Lustre)
WORKSPACE={{ shared_workspace }}
{% else %}
# Use node-local scratch
WORKSPACE=$SLURM_TMPDIR
{% endif %}

cd $WORKSPACE

# Copy required files
{% if manifest_file %}
cp {{ manifest_file }} $WORKSPACE/build_manifest.json
{% endif %}
{% if credential_file %}
cp {{ credential_file }} $WORKSPACE/credential.json
{% endif %}
{% if data_file %}
cp {{ data_file }} $WORKSPACE/data.json
{% endif %}

# =============================================================================
# Execute madengine Workflow
# =============================================================================

madengine run \
    {% if manifest_file %}--manifest-file build_manifest.json{% else %}--tags {{ tags }}{% endif %} \
    --timeout {{ timeout | default(3600) }} \
    {% if shared_data %}--force-mirror-local {{ shared_data }}{% endif %} \
    {% if live_output %}--live-output{% endif %}

EXIT_CODE=$?

# =============================================================================
# Collect Results
# =============================================================================

{% if results_dir %}
# Copy performance results to shared location
if [ -f "perf.csv" ]; then
    cp perf.csv {{ results_dir }}/perf_${SLURM_JOB_ID}_node${SLURM_NODEID}.csv
fi

# Copy logs
cp {{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_${SLURM_PROCID}.out \
   {{ results_dir }}/logs/ 2>/dev/null || true
{% endif %}

echo "Node $SLURM_NODEID completed with exit code $EXIT_CODE"
exit $EXIT_CODE

