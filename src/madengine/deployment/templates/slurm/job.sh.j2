#!/bin/bash
#SBATCH --job-name=madengine-{{ model_name }}
#SBATCH --output={{ output_dir }}/madengine-{{ model_name }}_%j_%t.out
#SBATCH --error={{ output_dir }}/madengine-{{ model_name }}_%j_%t.err
#SBATCH --partition={{ partition }}
#SBATCH --nodes={{ nodes }}
#SBATCH --ntasks={{ nodes }}
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node={{ gpus_per_node }}
#SBATCH --time={{ time_limit }}
{% if exclusive %}
#SBATCH --exclusive
{% endif %}
{% if qos %}
#SBATCH --qos={{ qos }}
{% endif %}
{% if account %}
#SBATCH --account={{ account }}
{% endif %}

# =============================================================================
# SLURM Job Configuration Generated by madengine-cli
# Model: {{ model_name }}
# Deployment: {{ nodes }} nodes x {{ gpus_per_node }} GPUs
# =============================================================================

# Load required modules
{% for module in modules %}
module load {{ module }}
{% endfor %}

# =============================================================================
# Environment Setup (Standard ML Environment Variables)
# =============================================================================

# Distributed training environment (auto-configured from SLURM)
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT={{ master_port | default(29500) }}
export WORLD_SIZE=$SLURM_NTASKS
export RANK=$SLURM_PROCID
export LOCAL_RANK=$SLURM_LOCALID
export NNODES={{ nodes }}
export GPUS_PER_NODE={{ gpus_per_node }}

# GPU visibility (ROCm/CUDA)
export ROCR_VISIBLE_DEVICES=$(seq -s, 0 $(({{ gpus_per_node }}-1)))
export CUDA_VISIBLE_DEVICES=$ROCR_VISIBLE_DEVICES

# Network configuration
{% if network_interface %}
export NCCL_SOCKET_IFNAME={{ network_interface }}
export GLOO_SOCKET_IFNAME={{ network_interface }}
{% endif %}

# Distributed backend configuration
{% if distributed_backend %}
export DISTRIBUTED_BACKEND={{ distributed_backend }}
{% endif %}

# Application-specific environment variables
{% for key, value in env_vars.items() %}
export {{ key }}="{{ value }}"
{% endfor %}

# madengine environment
export MAD_DEPLOYMENT_TYPE=slurm
export MAD_SLURM_JOB_ID=$SLURM_JOB_ID
export MAD_NODE_RANK=$SLURM_NODEID
export MAD_TOTAL_NODES={{ nodes }}

# =============================================================================
# Workspace Setup
# =============================================================================

# Determine workspace strategy based on configuration and node count
{% if shared_workspace %}
# Explicitly configured shared workspace (NFS/Lustre)
WORKSPACE={{ shared_workspace }}
WORKSPACE_TYPE="shared-explicit"
cd $WORKSPACE
{% else %}
{% if nodes > 1 %}
# =============================================================================
# Multi-node: Per-Node Setup (executed by srun on each task)
# =============================================================================
# For multi-node jobs, workspace setup must happen INSIDE srun context
# where SLURM_PROCID is properly set for each task.
# We'll create a setup script that srun will execute on each node.

echo "Multi-node deployment detected ({{ nodes }} nodes)"
echo "Per-node setup will be executed by srun on each task"
echo "Submission directory: {{ manifest_file | dirname }}"

# Note: Workspace setup happens later in srun context
# Skip to distributed training configuration
{% else %}
# Single-node: Prefer shared storage (submission dir), with local fallback if needed
# Check if submission directory is on shared filesystem
SUBMIT_DIR={{ manifest_file | dirname }}
if df -T "$SUBMIT_DIR" 2>/dev/null | grep -qE '\bnfs\b|\blustre\b|\bgpfs\b|\bceph\b'; then
    # Submission directory is on shared storage - use it directly (best practice)
    WORKSPACE=$SUBMIT_DIR
    WORKSPACE_TYPE="shared-nfs"
    echo "Using shared NFS workspace: $WORKSPACE"
else
    # Submission directory is local - use node scratch (rare case)
    if [ -n "$SLURM_TMPDIR" ] && [ -d "$SLURM_TMPDIR" ] && [ -w "$SLURM_TMPDIR" ]; then
        WORKSPACE=$SLURM_TMPDIR
        WORKSPACE_TYPE="local-slurm"
    else
        WORKSPACE=/tmp/madengine_job_${SLURM_JOB_ID:-$$}
        mkdir -p $WORKSPACE
        WORKSPACE_TYPE="local-tmp"
    fi
    echo "Using local node workspace: $WORKSPACE"
fi
{% endif %}
{% endif %}

{% if nodes > 1 %}
# Multi-node: Workspace setup happens in task script (executed by srun)
{% else %}
# Single-node: Setup workspace now
echo "Workspace type: $WORKSPACE_TYPE"
echo "Working directory: $WORKSPACE"
cd $WORKSPACE

# File handling based on workspace type
# Single-node: Use shared files if available, copy only if using local workspace
if [ "$WORKSPACE_TYPE" = "shared-nfs" ] || [ "$WORKSPACE_TYPE" = "shared-auto" ] || [ "$WORKSPACE_TYPE" = "shared-explicit" ]; then
    # Using shared workspace - reference files directly
    echo "Using files from shared storage (no copy needed)"
{% if manifest_file %}
    MANIFEST_FILE={{ manifest_file }}
{% endif %}
{% if credential_file %}
    CREDENTIAL_FILE={{ manifest_file | dirname }}/{{ credential_file }}
{% endif %}
{% if data_file %}
    DATA_FILE={{ manifest_file | dirname }}/{{ data_file }}
{% endif %}
else
    # Using local workspace - copy files from shared storage
    echo "Copying files to local workspace"
    SUBMIT_DIR={{ manifest_file | dirname }}
{% if manifest_file %}
    cp {{ manifest_file }} $WORKSPACE/build_manifest.json
    MANIFEST_FILE=$WORKSPACE/build_manifest.json
{% endif %}
{% if credential_file %}
    if [ -f "$SUBMIT_DIR/{{ credential_file }}" ]; then
        cp $SUBMIT_DIR/{{ credential_file }} $WORKSPACE/credential.json
        CREDENTIAL_FILE=$WORKSPACE/credential.json
    fi
{% endif %}
{% if data_file %}
    if [ -f "$SUBMIT_DIR/{{ data_file }}" ]; then
        cp $SUBMIT_DIR/{{ data_file }} $WORKSPACE/data.json
        DATA_FILE=$WORKSPACE/data.json
    fi
{% endif %}
fi
{% endif %}

{% if nodes == 1 %}
# =============================================================================
# Single-node: Verify madengine-cli availability
# =============================================================================

# Verify madengine-cli is available (or prepare fallback)
echo ""
echo "Verifying madengine-cli availability..."
if command -v madengine-cli >/dev/null 2>&1; then
    echo "  ✓ madengine-cli is available in PATH"
    MAD_CLI_VERSION=$(madengine-cli --version 2>/dev/null | head -n1 || echo "unknown")
    echo "  Version: $MAD_CLI_VERSION"
    export MAD_CLI_COMMAND="madengine-cli"
elif [ -f "$WORKSPACE/src/madengine/cli/app.py" ]; then
    echo "  ⚠ madengine-cli not found in PATH"
    echo "  Will use Python module fallback: python3 -m madengine.cli.app"
    export PYTHONPATH=$WORKSPACE/src:$PYTHONPATH
    export MAD_CLI_COMMAND="python3 -m madengine.cli.app"
else
    echo "  ❌ ERROR: madengine-cli not available and no source code found!"
    echo "  Cannot continue without madengine"
    exit 1
fi
echo ""

# =============================================================================
# Single-node: Create local execution manifest
# =============================================================================

{% if manifest_file %}
# Create a local-execution manifest by modifying deployment_config
ORIGINAL_MANIFEST="{{ manifest_file | basename }}"
LOCAL_MANIFEST="build_manifest_local.json"

echo "Creating local execution manifest from: $ORIGINAL_MANIFEST"

python3 -c "
import json
manifest_file = '$ORIGINAL_MANIFEST'
output_file = '$LOCAL_MANIFEST'
with open(manifest_file, 'r') as f:
    manifest = json.load(f)
if 'deployment_config' in manifest:
    gpus_per_node = None
    if 'slurm' in manifest['deployment_config']:
        gpus_per_node = manifest['deployment_config']['slurm'].get('gpus_per_node')
    manifest['deployment_config']['target'] = 'local'
    manifest['deployment_config'].pop('slurm', None)
    manifest['deployment_config'].pop('k8s', None)
    manifest['deployment_config'].pop('kubernetes', None)
    if gpus_per_node:
        manifest['deployment_config']['gpus_per_node'] = gpus_per_node
with open(output_file, 'w') as f:
    json.dump(manifest, f, indent=2)
print('Created local execution manifest')
"

if [ $? -eq 0 ]; then
    echo "✓ Forced local execution in manifest: $LOCAL_MANIFEST"
    EXEC_MANIFEST="$LOCAL_MANIFEST"
else
    echo "⚠ Failed to modify manifest, using original"
    EXEC_MANIFEST="$ORIGINAL_MANIFEST"
fi
{% else %}
EXEC_MANIFEST=""
{% endif %}
{% endif %}

# SLURM GPU Environment Check
# SLURM already sets CUDA_VISIBLE_DEVICES, ROCR_VISIBLE_DEVICES, GPU_DEVICE_ORDINAL
echo "SLURM GPU allocation:"
echo "  Allocated GPUs: ${SLURM_GPUS_ON_NODE:-unknown}"
echo "  CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-not set}"
echo "  ROCR_VISIBLE_DEVICES: ${ROCR_VISIBLE_DEVICES:-not set}"
echo "  Node: ${SLURM_NODEID}/${SLURM_NNODES} (Rank ${SLURM_PROCID}/${SLURM_NTASKS})"

# Set deployment environment flags
export MAD_IN_SLURM_JOB=1
export MAD_DEPLOYMENT_TYPE=slurm

# =============================================================================
# Configure Distributed Training Launcher
# =============================================================================
echo ""
echo "Distributed Training Configuration:"
echo "  NNODES: ${NNODES}"
echo "  GPUS_PER_NODE: ${GPUS_PER_NODE}"
echo "  TOTAL_GPUS: $((NNODES * GPUS_PER_NODE))"
echo "  MASTER_ADDR: ${MASTER_ADDR}"
echo "  MASTER_PORT: ${MASTER_PORT}"
echo "  NODE_RANK: ${SLURM_PROCID}"
echo "  WORLD_SIZE: ${WORLD_SIZE}"

# Configure MAD_MULTI_NODE_RUNNER for model scripts
# This environment variable will be passed to containers and used by model run scripts
if [ ${NNODES} -gt 1 ]; then
    # Multi-node distributed training: Use full torchrun with master coordination
    # IMPORTANT: SLURM_PROCID is evaluated HERE (on the host) before passing to Docker
    # because Docker containers don't have access to SLURM environment variables
    export MAD_MULTI_NODE_RUNNER="torchrun --nnodes=${NNODES} --nproc_per_node=${GPUS_PER_NODE} --node_rank=${SLURM_PROCID} --master_addr=${MASTER_ADDR} --master_port=${MASTER_PORT}"
    echo "  Launcher: torchrun (multi-node distributed)"
else
    # Single-node training (1 or more GPUs): Use standalone mode
    export MAD_MULTI_NODE_RUNNER="torchrun --standalone --nproc_per_node=${GPUS_PER_NODE}"
    echo "  Launcher: torchrun (single-node standalone)"
fi
echo "  MAD_MULTI_NODE_RUNNER: ${MAD_MULTI_NODE_RUNNER}"
echo ""

# Set network interface for NCCL/GLOO if not already set
{% if network_interface %}
export NCCL_SOCKET_IFNAME={{ network_interface }}
export GLOO_SOCKET_IFNAME={{ network_interface }}
{% else %}
# Try to auto-detect InfiniBand or high-speed network interface
if [ -z "${NCCL_SOCKET_IFNAME}" ]; then
    # Check for InfiniBand interfaces
    if ip link show | grep -q "ib[0-9]"; then
        export NCCL_SOCKET_IFNAME=ib0
        export GLOO_SOCKET_IFNAME=ib0
        echo "  Network: InfiniBand (ib0)"
    else
        # Fallback to first non-loopback interface
        DEFAULT_IFACE=$(ip route | grep default | awk '{print $5}' | head -n1)
        export NCCL_SOCKET_IFNAME=${DEFAULT_IFACE:-eth0}
        export GLOO_SOCKET_IFNAME=${DEFAULT_IFACE:-eth0}
        echo "  Network: ${NCCL_SOCKET_IFNAME}"
    fi
fi
{% endif %}

{% if nodes > 1 %}
# =============================================================================
# Multi-node: Execute with per-task setup
# =============================================================================
# Create a wrapper script that each srun task will execute
# This ensures workspace setup happens with correct SLURM_PROCID

TASK_SCRIPT="/tmp/madengine_task_${SLURM_JOB_ID}.sh"

cat > "$TASK_SCRIPT" << 'TASK_SCRIPT_EOF'
#!/bin/bash
set -e

echo "========================================================================="
echo "Task started on node: $(hostname)"
echo "SLURM_PROCID: ${SLURM_PROCID}"
echo "SLURM_JOB_ID: ${SLURM_JOB_ID}"
echo "========================================================================="

# Setup workspace (SLURM_PROCID is now available)
if [ -n "$SLURM_TMPDIR" ] && [ -d "$SLURM_TMPDIR" ] && [ -w "$SLURM_TMPDIR" ]; then
    WORKSPACE=$SLURM_TMPDIR/madengine_node_${SLURM_PROCID}
else
    WORKSPACE=/tmp/madengine_job_${SLURM_JOB_ID}_node_${SLURM_PROCID}
fi
mkdir -p $WORKSPACE
WORKSPACE_TYPE="local-multinode"
echo "Multi-node: Node ${SLURM_PROCID} using local workspace: $WORKSPACE"
cd $WORKSPACE

# Copy entire project to local workspace
echo "Copying entire project to local workspace"
SUBMISSION_DIR={{ manifest_file | dirname }}

echo "  Copying from: $SUBMISSION_DIR"
echo "  Copying to: $WORKSPACE"
rsync -a --quiet \
      --exclude='.git' \
      --exclude='__pycache__' \
      --exclude='*.pyc' \
      --exclude='**/__pycache__' \
      --exclude='*.egg-info' \
      --exclude='.pytest_cache' \
      --exclude='venv' \
      --exclude='.venv' \
      --exclude='env' \
      --exclude='.env' \
      --exclude='slurm_output/*.out' \
      --exclude='slurm_output/*.log' \
      "$SUBMISSION_DIR/" "$WORKSPACE/"

echo "  ✓ Project copied to local workspace"
echo ""

# Install madengine on work node
echo "Installing madengine on work node..."

if [ -f "$WORKSPACE/pyproject.toml" ] && grep -q '"madengine"' "$WORKSPACE/pyproject.toml"; then
    echo "  Detected: madengine package"
    echo "  Installing: pip install -e ."
    if python3 -m pip install --user -q -e "$WORKSPACE" >/dev/null 2>&1; then
        echo "  ✓ madengine installed"
    else
        echo "  ⚠ Warning: pip install failed"
    fi
elif [ -f "$WORKSPACE/requirements.txt" ]; then
    echo "  Detected: MAD package"
    echo "  Installing: pip install -r requirements.txt"
    if python3 -m pip install --user -q -r "$WORKSPACE/requirements.txt" >/dev/null 2>&1; then
        echo "  ✓ Dependencies installed"
    else
        echo "  ⚠ Warning: pip install failed"
    fi
elif [ -f "$WORKSPACE/setup.py" ]; then
    echo "  Detected: Package with setup.py"
    if python3 -m pip install --user -q -e "$WORKSPACE" >/dev/null 2>&1; then
        echo "  ✓ Package installed"
    else
        echo "  ⚠ Warning: pip install failed"
    fi
fi

# Verify madengine-cli availability
echo ""
if command -v madengine-cli >/dev/null 2>&1; then
    echo "✓ madengine-cli available"
    MAD_CLI_COMMAND="madengine-cli"
elif [ -f "$WORKSPACE/src/madengine/cli/app.py" ]; then
    echo "⚠ Using Python module fallback"
    export PYTHONPATH=$WORKSPACE/src:$PYTHONPATH
    MAD_CLI_COMMAND="python3 -m madengine.cli.app"
else
    echo "❌ ERROR: madengine-cli not available!"
    exit 1
fi

# Create local execution manifest
ORIGINAL_MANIFEST="{{ manifest_file | basename }}"
LOCAL_MANIFEST="build_manifest_local.json"

echo ""
echo "Creating local execution manifest: $LOCAL_MANIFEST"

python3 -c "
import json
manifest_file = '$ORIGINAL_MANIFEST'
output_file = '$LOCAL_MANIFEST'
with open(manifest_file, 'r') as f:
    manifest = json.load(f)
if 'deployment_config' in manifest:
    gpus_per_node = None
    if 'slurm' in manifest['deployment_config']:
        gpus_per_node = manifest['deployment_config']['slurm'].get('gpus_per_node')
    manifest['deployment_config']['target'] = 'local'
    manifest['deployment_config'].pop('slurm', None)
    manifest['deployment_config'].pop('k8s', None)
    manifest['deployment_config'].pop('kubernetes', None)
    if gpus_per_node:
        manifest['deployment_config']['gpus_per_node'] = gpus_per_node
with open(output_file, 'w') as f:
    json.dump(manifest, f, indent=2)
print('✓ Created local execution manifest')
"

if [ $? -eq 0 ] && [ -f "$LOCAL_MANIFEST" ]; then
    EXEC_MANIFEST="$LOCAL_MANIFEST"
    echo "✓ Manifest ready: $EXEC_MANIFEST"
else
    echo "⚠ Using original manifest"
    EXEC_MANIFEST="$ORIGINAL_MANIFEST"
fi

# Show configuration
echo ""
echo "Node ${SLURM_PROCID} ready:"
echo "  Workspace: $WORKSPACE"
echo "  Manifest: $EXEC_MANIFEST"
echo "  Command: $MAD_CLI_COMMAND"
echo ""

# Execute madengine-cli
echo "Executing madengine-cli in LOCAL mode..."
$MAD_CLI_COMMAND run \
    --manifest-file "$EXEC_MANIFEST" \
    --timeout {{ timeout | default(3600) }} \
    {% if shared_data %}--force-mirror-local {{ shared_data }}{% endif %} \
    {% if live_output %}--live-output{% endif %} \
    --additional-context "{
        'docker_env_vars': {
            'MASTER_ADDR': '${MASTER_ADDR}',
            'MASTER_PORT': '${MASTER_PORT}',
            'WORLD_SIZE': '${WORLD_SIZE}',
            'RANK': '${RANK}',
            'LOCAL_RANK': '${LOCAL_RANK}',
            'NNODES': '${NNODES}',
            'NPROC_PER_NODE': '${GPUS_PER_NODE}',
            'MAD_MULTI_NODE_RUNNER': '${MAD_MULTI_NODE_RUNNER}',
            'NCCL_SOCKET_IFNAME': '${NCCL_SOCKET_IFNAME}',
            'GLOO_SOCKET_IFNAME': '${GLOO_SOCKET_IFNAME}',
            'NCCL_DEBUG': 'INFO',
            'NCCL_IB_DISABLE': '0',
            'NCCL_NET_GDR_LEVEL': '5'
        }
    }"

TASK_EXIT=$?
echo ""
echo "Task completed with exit code: $TASK_EXIT"

# Copy results back to submission directory
if [ $TASK_EXIT -eq 0 ]; then
    RESULTS_DIR={{ manifest_file | dirname }}
    echo "Copying results back to: $RESULTS_DIR"
    
    if [ -f "$WORKSPACE/perf.csv" ]; then
        cp "$WORKSPACE/perf.csv" "$RESULTS_DIR/perf_node_${SLURM_PROCID}.csv" 2>/dev/null || true
        echo "  Copied: perf_node_${SLURM_PROCID}.csv"
    fi
    
    for log in "$WORKSPACE"/*.log; do
        if [ -f "$log" ]; then
            cp "$log" "$RESULTS_DIR/" 2>/dev/null || true
        fi
    done
    
    echo "  ✓ Results copied"
fi

exit $TASK_EXIT
TASK_SCRIPT_EOF

chmod +x "$TASK_SCRIPT"

echo "Launching tasks on {{ nodes }} nodes..."
srun bash "$TASK_SCRIPT"
EXIT_CODE=$?

# Cleanup task script
rm -f "$TASK_SCRIPT"

{% else %}
# =============================================================================
# Single-node: Execute directly
# =============================================================================
echo "Executing madengine in LOCAL mode (inside SLURM job)"
echo "  Command: $MAD_CLI_COMMAND"

$MAD_CLI_COMMAND run \
    {% if manifest_file %}--manifest-file "$EXEC_MANIFEST"{% else %}--tags {{ tags }}{% endif %} \
    --timeout {{ timeout | default(3600) }} \
    {% if shared_data %}--force-mirror-local {{ shared_data }}{% endif %} \
    {% if live_output %}--live-output{% endif %} \
    --additional-context "{
        'docker_env_vars': {
            'MASTER_ADDR': '${MASTER_ADDR}',
            'MASTER_PORT': '${MASTER_PORT}',
            'WORLD_SIZE': '${WORLD_SIZE}',
            'RANK': '${RANK}',
            'LOCAL_RANK': '${LOCAL_RANK}',
            'NNODES': '${NNODES}',
            'NPROC_PER_NODE': '${GPUS_PER_NODE}',
            'MAD_MULTI_NODE_RUNNER': '${MAD_MULTI_NODE_RUNNER}',
            'NCCL_SOCKET_IFNAME': '${NCCL_SOCKET_IFNAME}',
            'GLOO_SOCKET_IFNAME': '${GLOO_SOCKET_IFNAME}',
            'NCCL_DEBUG': 'INFO',
            'NCCL_IB_DISABLE': '0',
            'NCCL_NET_GDR_LEVEL': '5'
        }
    }"

EXIT_CODE=$?
{% endif %}

# =============================================================================
# Copy Results from Local Workspace to Shared Results Directory (Multi-node)
# =============================================================================

{% if nodes > 1 %}
# Multi-node: Copy results/logs from each work node back to shared results directory
# This allows collecting outputs from all nodes in one location
if [ "$WORKSPACE_TYPE" = "local-multinode" ]; then
    RESULTS_DIR={{ manifest_file | dirname }}
    echo "Copying results from local workspace to shared results directory..."
    
    # Copy performance results (per-node)
    if [ -f "$WORKSPACE/perf.csv" ]; then
        cp "$WORKSPACE/perf.csv" "$RESULTS_DIR/perf_node_${SLURM_PROCID}.csv" 2>/dev/null || true
        echo "  Copied: perf_node_${SLURM_PROCID}.csv"
    fi
    
    # Copy log files
    if ls "$WORKSPACE"/*.log >/dev/null 2>&1; then
        cp "$WORKSPACE"/*.log "$RESULTS_DIR/" 2>/dev/null || true
        echo "  Copied: log files"
    fi
    
    # Copy any output files that might be needed
    if ls "$WORKSPACE"/*.out >/dev/null 2>&1; then
        cp "$WORKSPACE"/*.out "$RESULTS_DIR/" 2>/dev/null || true
    fi
    
    echo "Results copied to: $RESULTS_DIR"
fi
{% endif %}

# =============================================================================
# Collect Results
# =============================================================================

{% if results_dir %}
# Copy performance results to shared location
if [ -f "perf.csv" ]; then
    cp perf.csv {{ results_dir }}/perf_${SLURM_JOB_ID}_node${SLURM_NODEID}.csv
fi

# Copy logs
cp {{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_${SLURM_PROCID}.out \
   {{ results_dir }}/logs/ 2>/dev/null || true
{% endif %}

echo "Node $SLURM_NODEID completed with exit code $EXIT_CODE"
exit $EXIT_CODE

