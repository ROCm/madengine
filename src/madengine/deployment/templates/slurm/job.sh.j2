#!/bin/bash
#SBATCH --job-name=madengine-{{ model_name }}
#SBATCH --output={{ output_dir }}/madengine-{{ model_name }}_%j_%t.out
#SBATCH --error={{ output_dir }}/madengine-{{ model_name }}_%j_%t.err
#SBATCH --partition={{ partition }}
#SBATCH --nodes={{ nodes }}
#SBATCH --ntasks={{ nodes }}
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node={{ gpus_per_node }}
#SBATCH --time={{ time_limit }}
{% if exclusive %}
#SBATCH --exclusive
{% endif %}
{% if qos %}
#SBATCH --qos={{ qos }}
{% endif %}
{% if account %}
#SBATCH --account={{ account }}
{% endif %}

# =============================================================================
# SLURM Job Configuration Generated by madengine-cli
# Model: {{ model_name }}
# Deployment: {{ nodes }} nodes x {{ gpus_per_node }} GPUs
# =============================================================================

# Load required modules
{% for module in modules %}
module load {{ module }}
{% endfor %}

# =============================================================================
# Environment Setup (Standard ML Environment Variables)
# =============================================================================

# Distributed training environment (auto-configured from SLURM)
export MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_PORT={{ master_port | default(29500) }}
export WORLD_SIZE=$SLURM_NTASKS
# NOTE: RANK is set per-task inside srun context (not here in main script)
# export RANK=$SLURM_PROCID  # <-- DO NOT SET HERE: will be 0 for all tasks
export LOCAL_RANK=$SLURM_LOCALID
export NNODES={{ nodes }}
export GPUS_PER_NODE={{ gpus_per_node }}

# GPU visibility (ROCm/CUDA)
export ROCR_VISIBLE_DEVICES=$(seq -s, 0 $(({{ gpus_per_node }}-1)))
export CUDA_VISIBLE_DEVICES=$ROCR_VISIBLE_DEVICES

# Network configuration
{% if network_interface %}
export NCCL_SOCKET_IFNAME={{ network_interface }}
export GLOO_SOCKET_IFNAME={{ network_interface }}
{% endif %}

# Distributed backend configuration
{% if distributed_backend %}
export DISTRIBUTED_BACKEND={{ distributed_backend }}
{% endif %}

# Application-specific environment variables
{% for key, value in env_vars.items() %}
export {{ key }}="{{ value }}"
{% endfor %}

# madengine environment
export MAD_DEPLOYMENT_TYPE=slurm
export MAD_SLURM_JOB_ID=$SLURM_JOB_ID
export MAD_NODE_RANK=$SLURM_NODEID
export MAD_TOTAL_NODES={{ nodes }}

# =============================================================================
# Workspace Setup
# =============================================================================

# Determine workspace strategy based on configuration and node count
{% if shared_workspace %}
# Explicitly configured shared workspace (NFS/Lustre)
WORKSPACE={{ shared_workspace }}
WORKSPACE_TYPE="shared-explicit"
cd $WORKSPACE
{% else %}
{% if nodes > 1 %}
# =============================================================================
# Multi-node: Per-Node Setup (executed by srun on each task)
# =============================================================================
# For multi-node jobs, workspace setup must happen INSIDE srun context
# where SLURM_PROCID is properly set for each task.
# We'll create a setup script that srun will execute on each node.

echo "Multi-node deployment detected ({{ nodes }} nodes)"
echo "Per-node setup will be executed by srun on each task"
echo "Submission directory: {{ manifest_file | dirname }}"

# Note: Workspace setup happens later in srun context
# Skip to distributed training configuration
{% else %}
# Single-node: Prefer shared storage (submission dir), with local fallback if needed
# Check if submission directory is on shared filesystem
SUBMIT_DIR={{ manifest_file | dirname }}
if df -T "$SUBMIT_DIR" 2>/dev/null | grep -qE '\bnfs\b|\blustre\b|\bgpfs\b|\bceph\b'; then
    # Submission directory is on shared storage - use it directly (best practice)
    WORKSPACE=$SUBMIT_DIR
    WORKSPACE_TYPE="shared-nfs"
    echo "Using shared NFS workspace: $WORKSPACE"
else
    # Submission directory is local - use node scratch (rare case)
    if [ -n "$SLURM_TMPDIR" ] && [ -d "$SLURM_TMPDIR" ] && [ -w "$SLURM_TMPDIR" ]; then
        WORKSPACE=$SLURM_TMPDIR
        WORKSPACE_TYPE="local-slurm"
    else
        WORKSPACE=/tmp/madengine_job_${SLURM_JOB_ID:-$$}
        mkdir -p $WORKSPACE
        WORKSPACE_TYPE="local-tmp"
    fi
    echo "Using local node workspace: $WORKSPACE"
fi
{% endif %}
{% endif %}

{% if nodes > 1 %}
# Multi-node: Workspace setup happens in task script (executed by srun)
{% else %}
# Single-node: Setup workspace now
echo "Workspace type: $WORKSPACE_TYPE"
echo "Working directory: $WORKSPACE"
cd $WORKSPACE

# File handling based on workspace type
# Single-node: Use shared files if available, copy only if using local workspace
if [ "$WORKSPACE_TYPE" = "shared-nfs" ] || [ "$WORKSPACE_TYPE" = "shared-auto" ] || [ "$WORKSPACE_TYPE" = "shared-explicit" ]; then
    # Using shared workspace - reference files directly
    echo "Using files from shared storage (no copy needed)"
{% if manifest_file %}
    MANIFEST_FILE={{ manifest_file }}
{% endif %}
{% if credential_file %}
    CREDENTIAL_FILE={{ manifest_file | dirname }}/{{ credential_file }}
{% endif %}
{% if data_file %}
    DATA_FILE={{ manifest_file | dirname }}/{{ data_file }}
{% endif %}
else
    # Using local workspace - copy files from shared storage
    echo "Copying files to local workspace"
    SUBMIT_DIR={{ manifest_file | dirname }}
{% if manifest_file %}
    cp {{ manifest_file }} $WORKSPACE/build_manifest.json
    MANIFEST_FILE=$WORKSPACE/build_manifest.json
{% endif %}
{% if credential_file %}
    if [ -f "$SUBMIT_DIR/{{ credential_file }}" ]; then
        cp $SUBMIT_DIR/{{ credential_file }} $WORKSPACE/credential.json
        CREDENTIAL_FILE=$WORKSPACE/credential.json
    fi
{% endif %}
{% if data_file %}
    if [ -f "$SUBMIT_DIR/{{ data_file }}" ]; then
        cp $SUBMIT_DIR/{{ data_file }} $WORKSPACE/data.json
        DATA_FILE=$WORKSPACE/data.json
    fi
{% endif %}
fi
{% endif %}

{% if nodes == 1 %}
# =============================================================================
# Single-node: Verify madengine-cli availability
# =============================================================================

# Verify madengine-cli is available (or prepare fallback)
echo ""
echo "Verifying madengine-cli availability..."
if command -v madengine-cli >/dev/null 2>&1; then
    echo "  âœ“ madengine-cli is available in PATH"
    MAD_CLI_VERSION=$(madengine-cli --version 2>/dev/null | head -n1 || echo "unknown")
    echo "  Version: $MAD_CLI_VERSION"
    export MAD_CLI_COMMAND="madengine-cli"
elif [ -f "$WORKSPACE/src/madengine/cli/app.py" ]; then
    echo "  âš  madengine-cli not found in PATH"
    echo "  Will use Python module fallback: python3 -m madengine.cli.app"
    export PYTHONPATH=$WORKSPACE/src:$PYTHONPATH
    export MAD_CLI_COMMAND="python3 -m madengine.cli.app"
else
    echo "  âŒ ERROR: madengine-cli not available and no source code found!"
    echo "  Cannot continue without madengine"
    exit 1
fi
echo ""

# =============================================================================
# Single-node: Create local execution manifest
# =============================================================================

{% if manifest_file %}
# Create a local-execution manifest by modifying deployment_config
ORIGINAL_MANIFEST="{{ manifest_file | basename }}"
LOCAL_MANIFEST="build_manifest_local.json"

echo "Creating local execution manifest from: $ORIGINAL_MANIFEST"

python3 -c "
import json
manifest_file = '$ORIGINAL_MANIFEST'
output_file = '$LOCAL_MANIFEST'
with open(manifest_file, 'r') as f:
    manifest = json.load(f)
if 'deployment_config' in manifest:
    gpus_per_node = None
    if 'slurm' in manifest['deployment_config']:
        gpus_per_node = manifest['deployment_config']['slurm'].get('gpus_per_node')
    manifest['deployment_config']['target'] = 'local'
    manifest['deployment_config'].pop('slurm', None)
    manifest['deployment_config'].pop('k8s', None)
    manifest['deployment_config'].pop('kubernetes', None)
    if gpus_per_node:
        manifest['deployment_config']['gpus_per_node'] = gpus_per_node
with open(output_file, 'w') as f:
    json.dump(manifest, f, indent=2)
print('Created local execution manifest')
"

if [ $? -eq 0 ]; then
    echo "âœ“ Forced local execution in manifest: $LOCAL_MANIFEST"
    EXEC_MANIFEST="$LOCAL_MANIFEST"
else
    echo "âš  Failed to modify manifest, using original"
    EXEC_MANIFEST="$ORIGINAL_MANIFEST"
fi
{% else %}
EXEC_MANIFEST=""
{% endif %}
{% endif %}

# SLURM GPU Environment Check
# SLURM already sets CUDA_VISIBLE_DEVICES, ROCR_VISIBLE_DEVICES, GPU_DEVICE_ORDINAL
echo "SLURM GPU allocation:"
echo "  Allocated GPUs: ${SLURM_GPUS_ON_NODE:-unknown}"
echo "  CUDA_VISIBLE_DEVICES: ${CUDA_VISIBLE_DEVICES:-not set}"
echo "  ROCR_VISIBLE_DEVICES: ${ROCR_VISIBLE_DEVICES:-not set}"
echo "  Node: ${SLURM_NODEID}/${SLURM_NNODES} (Rank ${SLURM_PROCID}/${SLURM_NTASKS})"

# Set deployment environment flags
export MAD_IN_SLURM_JOB=1
export MAD_DEPLOYMENT_TYPE=slurm

# =============================================================================
# Configure Distributed Training Launcher
# =============================================================================
echo ""
echo "Distributed Training Configuration:"
echo "  NNODES: ${NNODES}"
echo "  GPUS_PER_NODE: ${GPUS_PER_NODE}"
echo "  TOTAL_GPUS: $((NNODES * GPUS_PER_NODE))"
echo "  MASTER_ADDR: ${MASTER_ADDR}"
echo "  MASTER_PORT: ${MASTER_PORT}"
echo "  WORLD_SIZE: ${WORLD_SIZE}"
{% if nodes > 1 %}
echo "  Launcher: torchrun (multi-node distributed)"
echo "  MAD_MULTI_NODE_RUNNER: torchrun --nnodes={{ nodes }} --nproc_per_node={{ gpus_per_node }} --node_rank=\${SLURM_PROCID} --master_addr=\${MASTER_ADDR} --master_port={{ master_port | default(29500) }}"
{% else %}
echo "  Launcher: torchrun (single-node)"  
echo "  MAD_MULTI_NODE_RUNNER: torchrun --standalone --nproc_per_node={{ gpus_per_node }}"
{% endif %}
echo ""

# Note: For multi-node jobs, node-specific variables (RANK, NODE_RANK, MAD_MULTI_NODE_RUNNER)
# are set inside each task where SLURM_PROCID is properly available per-node

# Set network interface for NCCL/GLOO if not already set
{% if network_interface %}
export NCCL_SOCKET_IFNAME={{ network_interface }}
export GLOO_SOCKET_IFNAME={{ network_interface }}
{% else %}
# Try to auto-detect InfiniBand or high-speed network interface
if [ -z "${NCCL_SOCKET_IFNAME}" ]; then
    # Check for InfiniBand interfaces
    if ip link show | grep -q "ib[0-9]"; then
        export NCCL_SOCKET_IFNAME=ib0
        export GLOO_SOCKET_IFNAME=ib0
        echo "  Network: InfiniBand (ib0)"
    else
        # Fallback to first non-loopback interface
        DEFAULT_IFACE=$(ip route | grep default | awk '{print $5}' | head -n1)
        export NCCL_SOCKET_IFNAME=${DEFAULT_IFACE:-eth0}
        export GLOO_SOCKET_IFNAME=${DEFAULT_IFACE:-eth0}
        echo "  Network: ${NCCL_SOCKET_IFNAME}"
    fi
fi
{% endif %}

{% if nodes > 1 %}
# =============================================================================
# Multi-node: Execute with per-task setup
# =============================================================================
# For multi-node distributed training:
# 1. Each srun task runs on a separate node with unique SLURM_PROCID
# 2. All nodes participate in training via PyTorch DDP/torchrun
# 3. Global metrics are computed via all_reduce (identical on all nodes)
# 4. Only master node (SLURM_PROCID=0) collects/reports final metrics
#
# This approach follows PyTorch distributed training best practices:
# - Avoids duplicate data in perf.csv
# - Prevents race conditions in metric extraction
# - Ensures worker nodes exit cleanly after training
# =============================================================================

# Create a wrapper script that each srun task will execute
# This ensures workspace setup happens with correct SLURM_PROCID

# Use submission directory (shared filesystem) for task script
# /tmp is local to each node and won't be accessible by srun on other nodes
TASK_SCRIPT="{{ manifest_file | dirname }}/slurm_output/madengine_task_${SLURM_JOB_ID}.sh"

cat > "$TASK_SCRIPT" << 'TASK_SCRIPT_EOF'
#!/bin/bash
set -e

echo "========================================================================="
echo "Task started on node: $(hostname)"
echo "SLURM_PROCID: ${SLURM_PROCID}"
echo "SLURM_JOB_ID: ${SLURM_JOB_ID}"
echo "========================================================================="

# Configure MAD_MULTI_NODE_RUNNER for this specific node
# CRITICAL: This must be done HERE where SLURM_PROCID is unique for each task
{% if nodes > 1 %}
# Verify SLURM_PROCID is set
if [ -z "${SLURM_PROCID}" ]; then
    echo "ERROR: SLURM_PROCID not set! Cannot determine node rank."
    exit 1
fi

# Capture node rank explicitly
NODE_RANK=${SLURM_PROCID}
export NODE_RANK

# Build torchrun command with explicit node_rank
export MAD_MULTI_NODE_RUNNER="torchrun --nnodes={{ nodes }} --nproc_per_node={{ gpus_per_node }} --node_rank=${NODE_RANK} --master_addr=${MASTER_ADDR} --master_port={{ master_port | default(29500) }}"

# Debug output
echo "=========================================="
echo "ðŸ”§ Multi-node Distributed Training Setup"
echo "=========================================="
echo "  SLURM_PROCID: ${SLURM_PROCID}"
echo "  NODE_RANK: ${NODE_RANK}"
echo "  NNODES: {{ nodes }}"
echo "  NPROC_PER_NODE: {{ gpus_per_node }}"
echo "  MASTER_ADDR: ${MASTER_ADDR}"
echo "  MASTER_PORT: {{ master_port | default(29500) }}"
echo "  MAD_MULTI_NODE_RUNNER: ${MAD_MULTI_NODE_RUNNER}"
echo "=========================================="
{% else %}
export MAD_MULTI_NODE_RUNNER="torchrun --standalone --nproc_per_node={{ gpus_per_node }}"
echo "Single-node launcher: ${MAD_MULTI_NODE_RUNNER}"
{% endif %}
echo ""

# Setup workspace (SLURM_PROCID is now available)
if [ -n "$SLURM_TMPDIR" ] && [ -d "$SLURM_TMPDIR" ] && [ -w "$SLURM_TMPDIR" ]; then
    WORKSPACE=$SLURM_TMPDIR/madengine_node_${SLURM_PROCID}
else
    WORKSPACE=/tmp/madengine_job_${SLURM_JOB_ID}_node_${SLURM_PROCID}
fi
mkdir -p $WORKSPACE
WORKSPACE_TYPE="local-multinode"
echo "Multi-node: Node ${SLURM_PROCID} using local workspace: $WORKSPACE"
cd $WORKSPACE

# Copy entire project to local workspace
echo "Copying entire project to local workspace"
SUBMISSION_DIR={{ manifest_file | dirname }}

echo "  Copying from: $SUBMISSION_DIR"
echo "  Copying to: $WORKSPACE"
rsync -a --quiet \
      --exclude='.git' \
      --exclude='__pycache__' \
      --exclude='*.pyc' \
      --exclude='**/__pycache__' \
      --exclude='*.egg-info' \
      --exclude='.pytest_cache' \
      --exclude='venv' \
      --exclude='.venv' \
      --exclude='env' \
      --exclude='.env' \
      --exclude='slurm_output/*.out' \
      --exclude='slurm_output/*.log' \
      "$SUBMISSION_DIR/" "$WORKSPACE/"

echo "  âœ“ Project copied to local workspace"
echo ""

# Install madengine on work node
echo "Installing madengine on work node..."

if [ -f "$WORKSPACE/pyproject.toml" ] && grep -q '"madengine"' "$WORKSPACE/pyproject.toml"; then
    echo "  Detected: madengine package"
    echo "  Installing: pip install -e ."
    if python3 -m pip install --user -q -e "$WORKSPACE" >/dev/null 2>&1; then
        echo "  âœ“ madengine installed"
    else
        echo "  âš  Warning: pip install failed"
    fi
elif [ -f "$WORKSPACE/requirements.txt" ]; then
    echo "  Detected: MAD package"
    echo "  Installing: pip install -r requirements.txt"
    if python3 -m pip install --user -q -r "$WORKSPACE/requirements.txt" >/dev/null 2>&1; then
        echo "  âœ“ Dependencies installed"
    else
        echo "  âš  Warning: pip install failed"
    fi
elif [ -f "$WORKSPACE/setup.py" ]; then
    echo "  Detected: Package with setup.py"
    if python3 -m pip install --user -q -e "$WORKSPACE" >/dev/null 2>&1; then
        echo "  âœ“ Package installed"
    else
        echo "  âš  Warning: pip install failed"
    fi
fi

# Verify madengine-cli availability
echo ""
if command -v madengine-cli >/dev/null 2>&1; then
    echo "âœ“ madengine-cli available"
    MAD_CLI_COMMAND="madengine-cli"
elif [ -f "$WORKSPACE/src/madengine/cli/app.py" ]; then
    echo "âš  Using Python module fallback"
    export PYTHONPATH=$WORKSPACE/src:$PYTHONPATH
    MAD_CLI_COMMAND="python3 -m madengine.cli.app"
else
    echo "âŒ ERROR: madengine-cli not available!"
    exit 1
fi

# Create local execution manifest
ORIGINAL_MANIFEST="{{ manifest_file | basename }}"
LOCAL_MANIFEST="build_manifest_local.json"

echo ""
echo "Creating local execution manifest: $LOCAL_MANIFEST"

python3 -c "
import json
manifest_file = '$ORIGINAL_MANIFEST'
output_file = '$LOCAL_MANIFEST'
with open(manifest_file, 'r') as f:
    manifest = json.load(f)
if 'deployment_config' in manifest:
    gpus_per_node = None
    if 'slurm' in manifest['deployment_config']:
        gpus_per_node = manifest['deployment_config']['slurm'].get('gpus_per_node')
    manifest['deployment_config']['target'] = 'local'
    manifest['deployment_config'].pop('slurm', None)
    manifest['deployment_config'].pop('k8s', None)
    manifest['deployment_config'].pop('kubernetes', None)
    if gpus_per_node:
        manifest['deployment_config']['gpus_per_node'] = gpus_per_node
with open(output_file, 'w') as f:
    json.dump(manifest, f, indent=2)
print('âœ“ Created local execution manifest')
"

if [ $? -eq 0 ] && [ -f "$LOCAL_MANIFEST" ]; then
    EXEC_MANIFEST="$LOCAL_MANIFEST"
    echo "âœ“ Manifest ready: $EXEC_MANIFEST"
else
    echo "âš  Using original manifest"
    EXEC_MANIFEST="$ORIGINAL_MANIFEST"
fi

# Show configuration
echo ""
echo "Node ${SLURM_PROCID} ready:"
echo "  Workspace: $WORKSPACE"
echo "  Manifest: $EXEC_MANIFEST"
echo "  Command: $MAD_CLI_COMMAND"
echo ""

# Execute madengine-cli
echo "Executing madengine-cli in LOCAL mode..."

# Set RANK to node rank for this task (SLURM_PROCID)
export RANK=${SLURM_PROCID}

# Set environment variable to control metric collection
# Only master node (SLURM_PROCID=0) should collect and report metrics
if [ "${SLURM_PROCID}" = "0" ]; then
    export MAD_COLLECT_METRICS="true"
else
    export MAD_COLLECT_METRICS="false"
fi

# Export all environment variables that need to be passed to Docker
# This ensures they're inherited by the madengine-cli process and Docker containers
export MASTER_ADDR="${MASTER_ADDR}"
export MASTER_PORT="${MASTER_PORT}"
export WORLD_SIZE="${WORLD_SIZE}"
export NNODES="{{ nodes }}"
export GPUS_PER_NODE="{{ gpus_per_node }}"

# Debug: Show environment variables being passed
echo "Environment variables for Docker container:"
echo "  MASTER_ADDR: ${MASTER_ADDR}"
echo "  MASTER_PORT: ${MASTER_PORT}"
echo "  WORLD_SIZE: ${WORLD_SIZE}"
echo "  RANK (node rank): ${RANK}"
echo "  NODE_RANK: ${NODE_RANK}"
echo "  NNODES: ${NNODES}"
echo "  NPROC_PER_NODE: ${GPUS_PER_NODE}"
echo "  MAD_MULTI_NODE_RUNNER: ${MAD_MULTI_NODE_RUNNER}"
if [ "${SLURM_PROCID}" = "0" ]; then
    echo "  MAD_IS_MASTER_NODE: true (will collect performance metrics)"
else
    echo "  MAD_IS_MASTER_NODE: false (training only, no metric collection)"
fi
echo ""

# Create node-specific log files in results directory
RESULTS_DIR={{ manifest_file | dirname }}
NODE_LOG_OUT="${RESULTS_DIR}/slurm_output/madengine-{{ model_name }}_${SLURM_JOB_ID}_node_${SLURM_PROCID}.out"
NODE_LOG_ERR="${RESULTS_DIR}/slurm_output/madengine-{{ model_name }}_${SLURM_JOB_ID}_node_${SLURM_PROCID}.err"

echo "Node ${SLURM_PROCID} logs:"
echo "  stdout: ${NODE_LOG_OUT}"
echo "  stderr: ${NODE_LOG_ERR}"
echo ""

# Run madengine-cli with output redirected to node-specific log files
$MAD_CLI_COMMAND run \
    --manifest-file "$EXEC_MANIFEST" \
    --timeout {{ timeout | default(3600) }} \
    {% if shared_data %}--force-mirror-local {{ shared_data }}{% endif %} \
    {% if live_output %}--live-output{% endif %} \
    --additional-context "{
        'docker_env_vars': {
            'MASTER_ADDR': '${MASTER_ADDR}',
            'MASTER_PORT': '${MASTER_PORT}',
            'WORLD_SIZE': '${WORLD_SIZE}',
            'RANK': '${RANK}',
            'LOCAL_RANK': '0',
            'NNODES': '${NNODES}',
            'NPROC_PER_NODE': '${GPUS_PER_NODE}',
            'NODE_RANK': '${NODE_RANK}',
            'MAD_MULTI_NODE_RUNNER': '${MAD_MULTI_NODE_RUNNER}',
            'MAD_COLLECT_METRICS': '${MAD_COLLECT_METRICS}',
            'NCCL_SOCKET_IFNAME': '${NCCL_SOCKET_IFNAME}',
            'GLOO_SOCKET_IFNAME': '${GLOO_SOCKET_IFNAME}',
            'NCCL_DEBUG': 'INFO',
            'NCCL_IB_DISABLE': '0',
            'NCCL_NET_GDR_LEVEL': '5'
        },
        'skip_perf_collection': $([ '${SLURM_PROCID}' != '0' ] && echo 'true' || echo 'false')
    }" > "${NODE_LOG_OUT}" 2> "${NODE_LOG_ERR}"

TASK_EXIT=$?
echo ""
echo "Task completed with exit code: $TASK_EXIT"

# =============================================================================
# Multi-Node Result Collection (Best Practice: Master Node Only)
# =============================================================================
# For distributed training, only the master node (SLURM_PROCID=0) should
# collect and report performance metrics to avoid:
# - Duplicate data in perf.csv
# - Race conditions in metric extraction
# - Failures from non-master nodes trying to report identical global metrics
#
# This follows PyTorch distributed training best practices where only rank 0
# reports final metrics.
# =============================================================================

if [ $TASK_EXIT -eq 0 ]; then
    if [ "${SLURM_PROCID}" = "0" ]; then
        # Master node: Collect and report results
        RESULTS_DIR={{ manifest_file | dirname }}
        echo ""
        echo "========================================================================"
        echo "Master Node (SLURM_PROCID=0): Collecting results"
        echo "========================================================================"
        echo "Copying results back to: $RESULTS_DIR"
        
        # Copy performance results (main metric file)
        if [ -f "$WORKSPACE/perf.csv" ]; then
            cp "$WORKSPACE/perf.csv" "$RESULTS_DIR/perf.csv" 2>/dev/null || true
            echo "  âœ“ Copied: perf.csv (global metrics)"
        fi
        
        # Copy log files
        for log in "$WORKSPACE"/*.log; do
            if [ -f "$log" ]; then
                log_basename=$(basename "$log")
                cp "$log" "$RESULTS_DIR/${log_basename}" 2>/dev/null || true
                echo "  âœ“ Copied: ${log_basename}"
            fi
        done
        
        # Copy any training results files
        if [ -f "$WORKSPACE/training_results.txt" ]; then
            cp "$WORKSPACE/training_results.txt" "$RESULTS_DIR/" 2>/dev/null || true
            echo "  âœ“ Copied: training_results.txt"
        fi
        
        echo "  âœ“ Master node results collection complete"
        echo "========================================================================"
    else
        # Worker nodes: Exit cleanly without collecting results
        echo ""
        echo "========================================================================"
        echo "Worker Node (SLURM_PROCID=${SLURM_PROCID}): Exiting cleanly"
        echo "========================================================================"
        echo "  Note: Performance metrics collected by master node only (best practice)"
        echo "========================================================================"
    fi
else
    echo ""
    echo "========================================================================"
    echo "Task FAILED with exit code: $TASK_EXIT"
    echo "  Node: SLURM_PROCID=${SLURM_PROCID}"
    echo "========================================================================"
fi

exit $TASK_EXIT
TASK_SCRIPT_EOF

chmod +x "$TASK_SCRIPT"

echo "Launching tasks on {{ nodes }} nodes..."
srun bash "$TASK_SCRIPT"
EXIT_CODE=$?

# Cleanup task script
rm -f "$TASK_SCRIPT"

{% else %}
# =============================================================================
# Single-node: Execute directly
# =============================================================================
# Configure MAD_MULTI_NODE_RUNNER for single-node
export MAD_MULTI_NODE_RUNNER="torchrun --standalone --nproc_per_node={{ gpus_per_node }}"
export RANK=0  # Single node always has rank 0
export NODE_RANK=0

echo "=========================================="
echo "ðŸ”§ Single-node Training Setup"
echo "=========================================="
echo "  NPROC_PER_NODE: {{ gpus_per_node }}"
echo "  MAD_MULTI_NODE_RUNNER: ${MAD_MULTI_NODE_RUNNER}"
echo "=========================================="
echo ""

echo "Executing madengine in LOCAL mode (inside SLURM job)"
echo "  Command: $MAD_CLI_COMMAND"
echo ""

$MAD_CLI_COMMAND run \
    {% if manifest_file %}--manifest-file "$EXEC_MANIFEST"{% else %}--tags {{ tags }}{% endif %} \
    --timeout {{ timeout | default(3600) }} \
    {% if shared_data %}--force-mirror-local {{ shared_data }}{% endif %} \
    {% if live_output %}--live-output{% endif %} \
    --additional-context "{
        'docker_env_vars': {
            'MASTER_ADDR': '${MASTER_ADDR}',
            'MASTER_PORT': '${MASTER_PORT}',
            'WORLD_SIZE': '${WORLD_SIZE}',
            'RANK': '${RANK}',
            'LOCAL_RANK': '0',
            'NNODES': '${NNODES}',
            'NPROC_PER_NODE': '${GPUS_PER_NODE}',
            'NODE_RANK': '${NODE_RANK}',
            'MAD_MULTI_NODE_RUNNER': '${MAD_MULTI_NODE_RUNNER}',
            'NCCL_SOCKET_IFNAME': '${NCCL_SOCKET_IFNAME}',
            'GLOO_SOCKET_IFNAME': '${GLOO_SOCKET_IFNAME}',
            'NCCL_DEBUG': 'INFO',
            'NCCL_IB_DISABLE': '0',
            'NCCL_NET_GDR_LEVEL': '5'
        }
    }"

EXIT_CODE=$?
{% endif %}

# =============================================================================
# Job Completion
# =============================================================================
# Note: For multi-node jobs, only the master node (SLURM_PROCID=0) collects
# and reports performance metrics. This follows distributed training best
# practices where:
# - Global metrics are identical across all nodes (computed via all_reduce)
# - Only rank 0 should report to avoid duplicate/conflicting data
# - Worker nodes exit cleanly after training completes

echo ""
if [ $EXIT_CODE -eq 0 ]; then
    echo "========================================================================"
    echo "âœ… SLURM Job Completed Successfully"
    echo "========================================================================"
    echo "  Job ID: ${SLURM_JOB_ID}"
    echo "  Nodes: {{ nodes }}"
    echo "  GPUs per node: {{ gpus_per_node }}"
    echo "  Total GPUs: $(({{ nodes }} * {{ gpus_per_node }}))"
    echo "  Results: {{ manifest_file | dirname }}/perf.csv"
    {% if nodes > 1 %}
    echo ""
    echo "  ðŸ“‹ Individual Node Logs ({{ nodes }} nodes):"
    echo "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    for i in $(seq 0 $(({{ nodes }} - 1))); do
        NODE_OUT="{{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_node_${i}.out"
        NODE_ERR="{{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_node_${i}.err"
        if [ -f "$NODE_OUT" ]; then
            OUT_SIZE=$(du -h "$NODE_OUT" 2>/dev/null | cut -f1)
            ERR_SIZE=$(du -h "$NODE_ERR" 2>/dev/null | cut -f1)
            echo "  Node $i:"
            echo "    stdout: ${NODE_OUT} (${OUT_SIZE})"
            echo "    stderr: ${NODE_ERR} (${ERR_SIZE})"
        fi
    done
    {% endif %}
    echo "========================================================================"
else
    echo "========================================================================"
    echo "âŒ SLURM Job Failed"
    echo "========================================================================"
    echo "  Job ID: ${SLURM_JOB_ID}"
    echo "  Exit Code: $EXIT_CODE"
    {% if nodes > 1 %}
    echo ""
    echo "  ðŸ“‹ Check Individual Node Logs:"
    echo "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    for i in $(seq 0 $(({{ nodes }} - 1))); do
        NODE_OUT="{{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_node_${i}.out"
        NODE_ERR="{{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_node_${i}.err"
        if [ -f "$NODE_OUT" ] || [ -f "$NODE_ERR" ]; then
            echo "  Node $i: ${NODE_OUT}"
        fi
    done
    {% else %}
    echo "  Check logs: {{ output_dir }}/madengine-{{ model_name }}_${SLURM_JOB_ID}_*.out"
    {% endif %}
    echo "========================================================================"
fi

exit $EXIT_CODE

