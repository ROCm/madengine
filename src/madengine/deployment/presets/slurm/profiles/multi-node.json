{
  "_comment": "Multi-node SLURM profile - optimized for distributed training across nodes",
  "_description": "Configuration for multi-node distributed training on SLURM cluster",
  
  "slurm": {
    "nodes": 2,
    "gpus_per_node": 8,
    "time": "24:00:00"
  },
  
  "distributed": {
    "backend": "nccl",
    "port": 29500
  },
  
  "env_vars": {
    "NCCL_DEBUG": "WARN",
    "NCCL_DEBUG_SUBSYS": "INIT,NET",
    "NCCL_IB_DISABLE": "1",
    "NCCL_SOCKET_IFNAME": "eth0",
    "TORCH_NCCL_HIGH_PRIORITY": "1",
    "GPU_MAX_HW_QUEUES": "2",
    "TORCH_NCCL_ASYNC_ERROR_HANDLING": "1",
    "NCCL_TIMEOUT": "600",
    "HSA_ENABLE_SDMA": "0",
    "HSA_FORCE_FINE_GRAIN_PCIE": "1",
    "RCCL_ENABLE_HIPGRAPH": "0"
  }
}

